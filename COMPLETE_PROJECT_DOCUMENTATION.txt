================================================================================
DRAFTED.AI -- MULTI-STORY FLOOR PLAN GNN: COMPLETE PROJECT DOCUMENTATION
================================================================================

Document Version: 1.0
Date: February 15, 2026
Project: Multi-Story Floor Plan Coherence Scoring via Graph Neural Networks
Organization: Drafted.ai (Pre-seed $1.65M, December 2025)
Script: floorplan_gnn.py (~2660 lines of Python)
Runtime Environment: Python 3.10.17, PyTorch 2.9.1+cu128, CUDA 12.8, Tesla T4
Total Execution Time: Approximately 12 minutes end-to-end


================================================================================
TABLE OF CONTENTS
================================================================================

1.  PROJECT OVERVIEW AND MOTIVATION
2.  THE PROBLEM WE ARE SOLVING
3.  THE DATASET: CVC-FP
4.  COMPLETE PIPELINE ARCHITECTURE
5.  STAGE 1: SVG PARSING -- HOW AND WHY
6.  STAGE 2: SYNTHETIC MULTI-STORY GENERATION
7.  STAGE 3: GRAPH CONSTRUCTION
8.  STAGE 4: THE GNN MODEL ARCHITECTURE
9.  STAGE 5: TRAINING STRATEGY
9b. STAGE 6b: POST-TRAINING DIAGNOSTIC
10. STAGE 6: THE REFINEMENT AGENT
11. STAGE 7: EVALUATION METHODOLOGY
12. COMPLETE RESULTS AND ANALYSIS
13. VISUALIZATION OUTPUTS
14. EXPLAINABILITY DEEP DIVE
15. EVERY DESIGN DECISION AND ITS ALTERNATIVES
16. LIMITATIONS OF THE CURRENT PROTOTYPE
17. PRODUCTION READINESS ROADMAP
18. APPENDIX A: EXACT RUNTIME NUMBERS
19. APPENDIX B: FILE MANIFEST
20. APPENDIX C: HYPERPARAMETER REFERENCE


================================================================================
1. PROJECT OVERVIEW AND MOTIVATION
================================================================================

Drafted.ai generates custom home floor plans from user input. The company raised
$1.65M in pre-seed funding in December 2025. The current ML model can generate
single-story floor plans, but customers and the market increasingly demand
multi-story homes. The core challenge described by the team is: "Teach our ML
model how to generate multi-story outputs."

This project is a prototype that addresses that challenge by building a system
that can:

(a) Take a single-story floor plan and evaluate whether it can be coherently
    stacked into a multi-story building.
(b) Score the structural coherence across multiple independent dimensions
    (stair alignment, wall alignment, door sizing, egress compliance).
(c) Automatically identify and fix violations through an iterative refinement
    agent.
(d) Provide human-interpretable explanations for every score and every fix.

The prototype uses a Graph Attention Network (GAT) as the core ML model because
floor plans are naturally represented as graphs (rooms are nodes, doors and
walls create edges), and because GAT attention weights provide built-in
interpretability that is critical for a construction/architecture domain where
every decision must be explainable and auditable.


================================================================================
2. THE PROBLEM WE ARE SOLVING
================================================================================

When you stack two floors in a building, several structural constraints must
hold simultaneously:

STAIR ALIGNMENT: The staircase on floor 1 must line up vertically with the
staircase on floor 2. If the stairs are offset by even a few feet, the building
is structurally impossible. In our system, we measure this as the Intersection
over Union (IoU) of the stair bounding boxes across floors. An IoU above 0.8
means the stairs are well-aligned.

WALL ALIGNMENT: Load-bearing walls on the upper floor should sit directly above
load-bearing walls on the lower floor. If an upper wall sits over open space
below, the structure cannot support itself. We measure this as the fraction of
upper-floor walls whose bounding boxes overlap at least 80% with some
lower-floor wall.

DOOR SIZING: Building codes require minimum door widths (typically 36 inches for
accessibility). When floors are modified to accommodate stairs or structural
changes, doors can get squeezed. We check that every door opening is at least
36 pixels wide (at our dataset's approximate scale of 1 pixel per inch).

EGRESS COMPLIANCE: Fire codes require at least two independent exit paths from
every floor. When structural modifications remove or block doors, egress paths
can be eliminated. We count doors on the building's outer perimeter and require
at least two.

OVERALL COHERENCE: A weighted combination of all four dimensions. A floor pair
is considered "coherent" if its overall score exceeds 0.8 (80%).

The baseline approach -- naively stacking two random floors with no intelligence
-- produces an overall coherence of only 69.1% on our test set, with egress
compliance as low as 33.1%. Our GNN agent brings overall coherence to 80.1%,
with the largest improvement in egress compliance (33.1% to 76.1%).


================================================================================
3. THE DATASET: CVC-FP
================================================================================

3.1 WHAT IT IS

CVC-FP (Computer Vision Center Floor Plans) is a publicly available dataset of
122 annotated floor plan documents created by the Computer Vision Center at the
Universitat Autonoma de Barcelona. It was published alongside the SGT
(Structural Groundtruthing Tool) and is a standard benchmark for floor plan
analysis research.

3.2 WHERE THE DATA LIVES

All 122 SVG annotation files are stored at:
    /home/sujals2144/drafted/data/cvc_fp/raw_svgs/

Corresponding floor plan images (119 PNG files) are at:
    /home/sujals2144/drafted/data/cvc_fp/raw_images/

We use only the SVG annotations (not the raster images) because the SVGs
contain precise vectorized polygon coordinates and structural relation metadata
that raster images do not provide.

3.3 SVG FILE FORMAT

Each SVG file contains:

- Canvas dimensions: <width> and <height> tags (non-standard SVG, which is why
  we use regex parsing instead of an XML parser -- more on this below).

- 50 class definitions: Listed in <class> tags. These include structural classes
  (Wall, Room, Door, Window, Separation, Parking) and furniture classes
  (Sofa-1 through Sofa-9, Table-1 through Table-7, Sink-1 through Sink-14,
  Bidet, Tub, Oven, TV, Stairs-1, Text, Roof).

- Polygon annotations: Each structural element is a <polygon> tag with:
    class   -- The semantic label (e.g., "Wall", "Door")
    id      -- A unique integer identifier within the file
    fill    -- A color code (e.g., #AFD8F8 for walls, #F6BD0F for doors)
    points  -- Space-separated coordinate pairs (e.g., "485.5,195.5 510,200")

- Relation annotations: Five types of structural relationships:
    incident   -- Two elements that physically touch (e.g., wall-to-wall joints)
    access     -- A triple (Room, Door, Room) meaning "these two rooms are
                  connected through this door"
    neighbour  -- Two rooms that are adjacent
    surround   -- A room and the list of walls/doors/windows forming its boundary
    outerP     -- The elements forming the building's outer perimeter

3.4 DATASET STATISTICS (FROM OUR ACTUAL PARSE)

    Total files parsed:          122
    Total rooms:                 1,239
    Total walls:                 5,845
    Total doors:                 1,144
    Total windows:               1,200
    Average rooms per floor:     10.2
    Average walls per floor:     47.9
    Average doors per floor:     9.4
    Canvas sizes:                Range from ~1000x900 to 5671x7383 pixels

3.5 CRITICAL DISCOVERY: NO STAIRS EXIST IN THE DATA

Despite "Stairs-1" appearing in the 50-class header of every SVG file, zero
actual stair polygons exist anywhere in the dataset. We verified this by parsing
all 122 files. This is a critical finding because the entire multi-story
coherence task depends on stair alignment. Our solution: synthetically inject
stair polygons into the largest room of each floor plan (detailed in Section 6).

3.6 WHY CVC-FP AND NOT ANOTHER DATASET

We chose CVC-FP for several reasons:

(a) It has precise vectorized annotations (polygon coordinates), not just raster
    segmentation masks. This lets us compute exact geometric relationships like
    bounding box IoU and wall overlap percentages.

(b) It includes structural relation metadata (access, neighbour, surround,
    outerP) that directly maps to graph edges. Most other floor plan datasets
    only provide pixel-level labels.

(c) At 122 plans with an average of 80 structural elements each, it is large
    enough to generate thousands of synthetic training pairs but small enough
    to train on a single GPU in minutes.

ALTERNATIVES WE CONSIDERED:

- CubiCasa5K: 5,000 floor plans with SVG annotations. Much larger, but its
  SVG format uses different annotation conventions and does not include the
  rich relation metadata (access triples, surround chains) that CVC-FP provides.
  For a prototype, CVC-FP's relation data is more valuable than CubiCasa5K's
  scale.

- ROBIN/ROBIN++: We have this dataset in the data folder (510 floor plans), but
  it only provides raster JPEG images without vectorized polygon annotations.
  We cannot compute precise geometric coherence metrics from raster images.

- R-FP (Rakuten Floor Plans): Another benchmark, but not publicly available
  without a data agreement.

- Synthetic-only: We could generate floor plans entirely from scratch using
  procedural generation. However, using real floor plans as the base ensures
  our model learns from realistic room layouts, wall thicknesses, and door
  placements that reflect actual architectural practice.


================================================================================
4. COMPLETE PIPELINE ARCHITECTURE
================================================================================

The system is a single Python script (floorplan_gnn.py) that executes a
10-stage pipeline:

    SVG Files (122)
        |
        v  [parse_svg() -- regex-based parser]
    FloorLayout Objects (122)
        |
        v  [inject_synthetic_stair() + create_positive/negative/mixed_pair()]
    MultiStoryPair Objects (2,000)
        |
        v  [build_pair_graph() -- encode nodes, edges, cross-floor links]
    PyTorch Geometric Data Objects (2,000)
        |
        |-- x: [N, 32]         -- 32-dimensional node features
        |-- edge_index: [2, E] -- edge connectivity
        |-- edge_attr: [E, 7]  -- 7-dimensional one-hot edge type
        |-- floor_mask: [N]    -- which floor each node belongs to
        |-- y_*: float         -- 5 target scores
        |
        v  [70/15/15 train/val/test split, SEED=42]
        |
        v  [train_model() -- 58 epochs, Adam, ReduceLROnPlateau, early stopping]
    Trained FloorPlanGAT Model (871,685 parameters)
        |
        v  [refinement_loop() -- 3 iterations per broken pair]
        |-- Score with model
        |-- Identify weakest dimension
        |-- Apply targeted heuristic fix
        |-- Re-score and repeat
        |
        v  [Evaluation + Visualization]
    Outputs:
        - Markdown evaluation table
        - Per-head score breakdown
        - Training curves (PNG)
        - Floor plan pair plots with radar charts (PNG)
        - GAT attention heatmaps (PNG)
        - Interactive SVGs with hover tooltips
        - Animated refinement GIFs
        - Saved model checkpoint (best_model.pt)


================================================================================
5. STAGE 1: SVG PARSING -- HOW AND WHY
================================================================================

5.1 WHY REGEX INSTEAD OF AN XML PARSER

The CVC-FP SVG format is non-standard XML. It contains bare tags like <width>,
<height>, <class>, and <relation> that are not valid SVG or XML elements. A
standard XML parser (like Python's xml.etree.ElementTree or lxml) would either
choke on these tags or require extensive preprocessing to handle them. We tested
both approaches early in development:

- xml.etree.ElementTree: Raised parsing errors because <width>1094</width>
  is not a valid SVG element.
- lxml with recover=True: Parsed but silently dropped the non-standard elements,
  losing critical metadata like canvas dimensions and structural relations.

Regex parsing with re.finditer() handles the non-standard format cleanly and
extracts exactly the data we need. The trade-off is that regex is brittle if
the SVG format changes, but since CVC-FP is a fixed, published dataset, this
is acceptable.

ALTERNATIVE: Preprocess the SVG files to convert them to valid XML first
(e.g., wrap bare tags in proper SVG namespace elements), then use an XML parser.
This would be more robust for a production system but adds complexity for no
benefit when the dataset is fixed.

5.2 WHAT WE PARSE

For each SVG file, we extract:

- Canvas dimensions (width, height) for coordinate normalization.
- Every polygon with class "Wall", "Door", "Window", "Room", "Separation", or
  "Parking". We skip furniture polygons (Sofa, Table, Sink, etc.) because they
  are irrelevant to structural coherence.
- For each polygon: its integer ID, class label, vertex coordinates, bounding
  box, centroid (average of vertices), area (shoelace formula), and bbox
  width/height.
- All five types of structural relations, parsed into specific data structures:
    access_triples: List of (room_id, door_id, room_id) tuples
    neighbour_pairs: List of (room_id, room_id) tuples
    surround_map: Dict mapping room_id to list of boundary polygon IDs
    outer_perimeter_ids: List of polygon IDs forming the outer wall chain

5.3 CLASS NAME NORMALIZATION

The dataset has 50 class names, many of which are variants (Wall, Wall-1,
Wallieee; Door, Door-1, Door-23; Sink-1 through Sink-14). We collapse these
into 7 canonical types:

    Raw Class Name          Canonical Type
    -----------------       --------------
    Wall, Wall-1, Wallieee  Wall
    Room                    Room
    Door, Door-1, Door-23   Door
    Window                  Window
    Separation              Separation
    Parking                 Parking
    Stairs-1                Stair (synthetic only)
    Everything else         Other (skipped)

5.4 GEOMETRY COMPUTATION

For each polygon, we compute:

- Bounding Box: The axis-aligned rectangle enclosing all vertices.
  (x_min, y_min, x_max, y_max). Used for IoU calculations and feature encoding.

- Centroid: Simple average of all vertex coordinates. We use the arithmetic mean
  rather than the area-weighted centroid (which uses the shoelace formula) because
  the difference is negligible for the roughly convex polygons in this dataset,
  and the simple mean is faster to compute.

- Area: Computed via the shoelace formula (sum of cross products of consecutive
  vertex pairs, divided by 2, absolute value). We take the absolute value to be
  orientation-independent (some polygons have clockwise vertex ordering, others
  counter-clockwise).

5.5 PARSE RESULTS

Our parser successfully processed all 122 files with zero failures. The total
structural element count:

    Rooms:       1,239
    Walls:       5,845
    Doors:       1,144
    Windows:     1,200
    Separations: (included in walls for structural analysis)
    Parking:     (included as a room variant)


================================================================================
6. STAGE 2: SYNTHETIC MULTI-STORY GENERATION
================================================================================

6.1 WHY SYNTHETIC DATA

The CVC-FP dataset contains only single-story floor plans. There are no
multi-story buildings in the dataset. Furthermore, as noted above, there are
zero actual stair polygons in the data. To train a model that scores
multi-story coherence, we must synthesize the training data.

This is a form of self-supervised learning: we construct examples where we
know the ground truth by design, then train the model to distinguish coherent
from incoherent stacking.

ALTERNATIVE APPROACHES:

- Collect real multi-story floor plan data: This would be ideal but is expensive
  and time-consuming. Multi-story datasets with per-floor vectorized annotations
  are extremely rare. For a prototype, synthetic data is the pragmatic choice.

- Use a generative model (VAE/GAN) to create floor plans: This adds enormous
  complexity and requires its own training pipeline. It also produces floor
  plans of uncertain quality that may not reflect real construction practices.
  Using real CVC-FP plans as the base ensures architectural realism.

6.2 STAIR INJECTION

Since no stairs exist in the data, we inject a synthetic stair polygon into
each floor plan before creating pairs. The injection process:

(a) Find the largest room by area. Stairs typically occupy significant space
    and are placed in major circulation areas, so the largest room is a
    reasonable default.

(b) Size the stair between 2% and 8% of total floor area. This range reflects
    real stair footprints: a typical residential stair is about 3 feet wide
    and 10-12 feet long (30-36 square feet), which in a 1,500 sq ft floor is
    about 2-2.4%. Commercial buildings can have larger stairs up to 5-8%.

(c) Use a 1:2 width-to-height aspect ratio. Standard stairs are roughly twice
    as long as they are wide.

(d) Place the stair inside the largest room's bounding box with a 10-pixel
    margin from the edges. The exact position within the room is randomized.

(e) Assign the stair a unique polygon ID and add it to the layout's polygon
    dictionary.

ALTERNATIVE: Place stairs at room intersections (near doors connecting multiple
rooms), which is more architecturally accurate for circulation cores. We chose
the simpler "largest room" heuristic because it guarantees the stair fits
without overlapping other elements, and because this is a prototype.

6.3 PAIR GENERATION

We generate 2,000 multi-story pairs in three categories:

POSITIVE PAIRS (33%, approximately 660 pairs):
  - Take a floor plan with an injected stair.
  - Deep-copy it to create the upper floor.
  - Apply tiny random perturbations to the upper floor: each polygon shifts
    by 0-3 pixels in a random direction.
  - This simulates real construction tolerances (minor alignment variations
    between floors that do not affect structural integrity).
  - Ground truth: All coherence scores should be high (stair IoU close to 1.0,
    wall alignment close to 1.0, doors still compliant, egress preserved).
  - Label: 1.0

NEGATIVE PAIRS (34%, approximately 680 pairs):
  Four deliberate violations are applied to the upper floor:
  (a) Large stair shift: 50-100 pixels in a random direction, breaking stair
      alignment. This represents a stair that is offset by 5-10 feet.
  (b) Wall removal: 20-40% of walls are randomly deleted from the upper floor,
      breaking structural alignment.
  (c) Door shrinkage: Random doors are scaled down to 30-70% of original size,
      potentially violating the 36-pixel minimum.
  (d) Egress removal: With walls removed and doors shrunk, egress paths may
      be eliminated.
  - Label: 0.0

MIXED PAIRS (33%, approximately 660 pairs):
  - Only 1-2 mild violations are applied:
    (a) Moderate stair shift: 20-40 pixels (2-4 feet).
    (b) Mild wall removal: 5-15% of walls.
  - Label: Computed as the average of the four coherence scores, typically
    falling between 0.3 and 0.7.
  - Purpose: Prevent the model from learning a binary shortcut. Without mixed
    pairs, the model could simply learn "high score = positive, low score =
    negative" without understanding the individual dimensions.

WHY THIS RATIO:
A roughly balanced 33/34/33 split ensures the model sees enough examples of
each category. The slight overweight on negatives (34%) ensures the model is
exposed to more violation patterns, which is important because negative pairs
have higher variance (different combinations of violations).

ACTUAL GENERATION RESULTS:
    Total pairs generated:   2,000
    Positive (label > 0.8):  1,006
    Negative (label < 0.2):  680
    Mixed (in between):      314
    Generation time:         26.8 seconds

The positive count is higher than expected (1006 vs. ~660) because some mixed
pairs ended up with high scores when only mild violations were applied. This
is actually desirable because it creates a smooth label distribution rather than
a sharp binary split.

6.4 GROUND TRUTH SCORE COMPUTATION

For each pair, we compute ground truth scores analytically (not via the model):

- Stair IoU: Axis-aligned bounding box IoU between lower and upper stair
  polygons. Ranges from 0 (no overlap) to 1 (perfect alignment).

- Wall Alignment: For each wall on the upper floor, find the best-matching wall
  on the lower floor by bbox IoU. Count the fraction of upper walls where the
  best match has IoU >= 0.80. We use 0.80 as the threshold because walls are
  thick elements (typically 20-30 pixels wide in the dataset) and perfect pixel
  alignment is unrealistic.

- Door Compliance: Fraction of upper-floor doors whose opening width (the
  shorter bbox dimension) is at least 36 pixels.

- Egress Compliance: 1.0 if the upper floor has at least 2 doors on the outer
  perimeter (or adjacent to outer-perimeter walls), 0.0 otherwise.


================================================================================
7. STAGE 3: GRAPH CONSTRUCTION
================================================================================

7.1 WHY GRAPHS

Floor plans are naturally graph-structured. Rooms are nodes. Doors create edges
between rooms. Walls form the boundary chain around each room. This structure
is explicitly encoded in the CVC-FP SVG relations (access triples give us the
room-door-room connectivity graph, neighbour pairs give adjacency, surround
chains give room boundaries).

A graph representation allows the GNN to reason about structural relationships
that would be lost in a flat feature vector or an image-based approach. For
example, "the stair on floor 1 is connected to the stair on floor 2 via a
cross-floor edge, and the model can propagate information along that edge to
compare stair positions."

ALTERNATIVE: Image-based approach using a CNN on rasterized floor plans. This
would work for visual similarity but cannot easily reason about structural
relationships (which room connects to which, which walls are load-bearing).
A graph approach encodes these relationships explicitly.

ALTERNATIVE: Flatten all polygon features into a fixed-size vector and use a
standard MLP. This loses the relational structure entirely. An MLP cannot
distinguish "wall A is adjacent to wall B" from "wall A is far from wall B"
unless the adjacency is explicitly encoded as a feature, which is essentially
what a graph does.

7.2 NODE FEATURES (32 DIMENSIONS)

Each polygon (room, wall, door, window, separation, parking, stair) becomes a
node in the graph. Its 32-dimensional feature vector encodes:

Dimensions 0-6: One-hot node type (7 classes)
    This tells the model what kind of element the node represents. Using one-hot
    encoding rather than a single integer is standard practice because neural
    networks interpret one-hot vectors as categorical features, whereas an
    integer (like 3 for "Window") would imply an ordinal relationship that does
    not exist.

Dimensions 7-10: Normalized bounding box (x_min/W, y_min/H, x_max/W, y_max/H)
    All coordinates are divided by canvas width (W) or height (H) to make them
    scale-invariant. Without normalization, a floor plan on a 1000x900 canvas
    and one on a 5000x4000 canvas would have very different feature magnitudes,
    confusing the model.

Dimensions 11-12: Normalized centroid (cx/W, cy/H)
    The center of mass of the polygon, used by the model to assess spatial
    positioning (e.g., "is this stair in the center or the corner?").

Dimensions 13-14: Normalized size (width/W, height/H)
    Relative size of the element within the floor plan.

Dimension 15: Normalized area (area / (W*H))
    Total area as a fraction of the canvas. This is important for rooms (large
    rooms may be hallways or living rooms) and for stairs (which should be
    2-8% of floor area).

Dimension 16: Aspect ratio (min(w,h) / max(w,h))
    Ranges from 0 (infinitely elongated) to 1 (square). Walls have low aspect
    ratios (long and thin), rooms have moderate ratios, stairs have ~0.5
    (1:2 ratio).

Dimensions 17-18: Position relative to canvas center
    ((cx - W/2)/W, (cy - H/2)/H). This helps the model reason about whether an
    element is central or peripheral, which is relevant for egress (exterior
    doors are on the periphery).

Dimension 19: Vertex count (n_vertices / 20, clamped to 1.0)
    Walls typically have 4 vertices (rectangles), doors have 5 (fan shapes),
    rooms can have many more (complex polygons). This helps the model
    distinguish element types beyond the one-hot encoding.

Dimensions 20-23: Fourier position encoding
    sin(pi*cx/W), cos(pi*cx/W), sin(pi*cy/H), cos(pi*cy/H). These provide
    smooth, periodic spatial features that help the model capture position more
    expressively than raw normalized coordinates. This is inspired by the
    positional encodings used in Transformers and NeRF.

Dimensions 24-27: Wall-specific features
    is_horizontal (1 if width > 2*height, else 0), is_vertical (1 if
    height > 2*width, else 0), normalized thickness (min dimension / W),
    normalized length (max dimension / W). These are zero for non-wall nodes.

Dimensions 28-29: Door-specific features
    Normalized opening width (min dimension / W), plus a reserved dimension.
    Zero for non-door nodes.

Dimensions 30-31: Room-specific features
    Area rank (0 = largest room, 1 = smallest, normalized), number of boundary
    elements in the surround chain (from the surround relation, normalized by
    dividing by 20). Zero for non-room nodes.

WHY 32 DIMENSIONS:
32 is a power of 2 (efficient for GPU memory alignment) and provides enough
capacity to encode both categorical (type), geometric (position, size, shape),
and relational (rank, boundary count) information without being so large that
it overfits on our relatively small dataset (2000 pairs, ~157 nodes per graph).

ALTERNATIVE: Use higher-dimensional features (64 or 128) with additional
properties like polygon perimeter, convexity, orientation angle, etc. We chose
32 to keep the model lightweight (<8 GB VRAM) and because additional features
showed diminishing returns in preliminary experiments.

7.3 EDGE CONSTRUCTION

Edges come from six sources, each encoded as a one-hot type vector (7 classes):

TYPE 0 -- SPATIAL PROXIMITY:
    For any two nodes within 10% of the canvas diagonal whose centroids are
    close but who have no other edge connecting them. This catches implicit
    spatial relationships not captured by the SVG relations.

TYPE 1 -- INCIDENT:
    Directly from SVG <relation type="incident"> tags. These connect elements
    that physically touch (wall-to-wall joints, wall-to-door connections).
    Bidirectional. This is the densest edge type (9,863 incident relations
    across the dataset).

TYPE 2 -- ACCESS:
    From SVG <relation type="access"> triples. For each (Room A, Door D,
    Room B) triple, we create six edges: A<->D, B<->D, A<->B. This fully
    encodes the room connectivity graph with doors as intermediaries.

TYPE 3 -- NEIGHBOUR:
    From SVG <relation type="neighbour"> pairs. Room-to-room adjacency edges,
    bidirectional. These capture rooms that share a wall even if there is no
    door between them.

TYPE 4 -- SURROUND:
    From SVG <relation type="surround"> lists. For each room, create edges
    from the room to every element in its boundary chain (walls, doors,
    windows) and vice versa. This tells the model "these walls form room X's
    perimeter."

TYPE 5 -- CROSS-FLOOR:
    Added during pair graph construction. For each pair of nodes across
    floors that have the same class (e.g., both are Walls) and whose centroids
    are within 10% of the canvas diagonal, we add a cross-floor edge. These
    edges are critical: without them, the two floors would be disconnected
    subgraphs and the GNN could not reason about inter-floor relationships.

TYPE 6 -- STAIR LINK:
    A dedicated edge type connecting the stair on floor 1 to the stair on
    floor 2. This is the most semantically important edge because stair
    alignment is one of the four primary coherence dimensions.

WHY ONE-HOT EDGE TYPES INSTEAD OF SCALAR:
Using a 7-dimensional one-hot vector for edge types allows the GAT attention
mechanism to attend differently to each type. A scalar encoding (e.g., 0-6)
would imply an ordinal relationship between edge types that does not exist
(incident is not "greater than" access). The one-hot encoding treats each
type as an independent category, which is the correct inductive bias.

7.4 PAIR GRAPH CONSTRUCTION

When building the graph for a multi-story pair:

(a) Build the lower floor graph: nodes indexed [0, N1-1].
(b) Build the upper floor graph: nodes indexed [N1, N1+N2-1] (offset by N1).
(c) Add cross-floor edges (types 5 and 6) between the two subgraphs.
(d) Store a floor_mask tensor: 0 for lower floor nodes, 1 for upper floor nodes.
(e) Attach the five ground-truth target scores (stair IoU, wall alignment,
    door compliance, egress, overall label).

GRAPH STATISTICS FROM OUR ACTUAL BUILD:
    Total graphs built:       2,000
    Build time:               108.0 seconds
    Average nodes per graph:  157 (roughly 80 per floor * 2)
    Average edges per graph:  3,978


================================================================================
8. STAGE 4: THE GNN MODEL ARCHITECTURE
================================================================================

8.1 WHY GAT (GRAPH ATTENTION NETWORK)

We chose GAT over the alternatives for three reasons:

(a) INTERPRETABILITY: GAT computes attention weights for every edge at every
    layer. These weights tell us "how much does the model attend to this edge
    when computing the node's representation." We can visualize these weights
    to show, for example, that the model attends heavily to stair-link edges
    when computing the stair alignment score. This is critical for a
    construction/architecture domain where stakeholders need to understand
    why the model gave a particular score.

(b) EDGE-TYPE AWARENESS: PyTorch Geometric's GATConv supports an edge_dim
    parameter that lets edge attributes modulate the attention computation.
    This means the model can learn to pay more attention to certain edge types
    (e.g., stair_link edges when scoring stair alignment) without us having to
    hard-code this behavior.

(c) PROVEN EFFECTIVENESS: GAT has been shown to outperform GCN on heterogeneous
    graphs where different edge types carry different semantics. Our graph has
    7 edge types, making it a strongly heterogeneous graph.

ALTERNATIVES WE CONSIDERED:

- GCNConv (Graph Convolutional Network): Simpler and faster than GAT, but
  treats all edges equally (no attention mechanism). This means a stair-link
  edge would be weighted the same as a spatial-proximity edge, which is
  clearly wrong. GCN also does not provide attention weights for
  interpretability.

- GraphSAGE: Good for large-scale inductive learning, but we have a small
  dataset (2000 graphs) and do not need inductive generalization to unseen
  node types. GraphSAGE's sampling mechanism adds complexity without benefit
  here.

- Graph Transformer: The most expressive option, computing full self-attention
  over all nodes. However, this has O(N^2) complexity where N is the number
  of nodes. With 157 nodes per graph, each forward pass would compute 157^2 =
  24,649 attention pairs, compared to GAT's E attention computations (where
  E is the number of edges, ~3,978). Graph Transformers are overkill for our
  graph sizes and would not fit in 8 GB VRAM with batched training.

- Heterogeneous GNN (HeteroConv): PyTorch Geometric supports explicitly
  heterogeneous models that maintain separate weight matrices for each node
  type and edge type. This is more principled than our approach of encoding
  types as features, but it requires significantly more code and more
  parameters. For a prototype with only 7 node types and 7 edge types, the
  feature-encoding approach is simpler and sufficient.

8.2 MODEL ARCHITECTURE DETAILS

LAYER STRUCTURE:

    Input: 32-dim node features
        |
        v [Linear projection: 32 -> 256]
        |
        v [GAT Layer 1: 256 -> 64*4=256, 4 heads, edge_dim=7]
        |  + LayerNorm + ELU + Dropout(0.2) + Residual Connection
        |
        v [GAT Layer 2: 256 -> 64*4=256, 4 heads, edge_dim=7]
        |  + LayerNorm + ELU + Dropout(0.2) + Residual Connection
        |
        v [GAT Layer 3: 256 -> 64*4=256, 4 heads, edge_dim=7]
        |  + LayerNorm + ELU + Dropout(0.2) + Residual Connection
        |
        v [Floor-Aware Global Pooling]
        |  Lower floor: mean_pool + max_pool = 512-dim
        |  Upper floor: mean_pool + max_pool = 512-dim
        |  Concatenate: 1024-dim
        |
        v [5 Scoring Heads, each: Linear(1024,128) -> ReLU -> Dropout -> Linear(128,1) -> Sigmoid]
        |
        Output: 5 scores in [0, 1]

COMPONENT-BY-COMPONENT EXPLANATION:

Input Projection (Linear 32 -> 256):
    The raw 32-dim features are projected to the GAT's internal dimension
    (64 per head * 4 heads = 256). This projection also serves as a learnable
    feature transformation that the GAT layers can build upon.

GAT Layers (3 layers, 4 heads each):
    Each layer computes multi-head attention over the graph. With 4 heads, the
    model can attend to 4 different aspects of each edge simultaneously. For
    example, one head might specialize in structural relationships (incident
    edges), another in spatial proximity, and another in cross-floor
    connections.

    The edge_dim=7 parameter means each GAT layer receives the 7-dimensional
    one-hot edge type vector and uses it to modulate the attention computation.
    Specifically, PyG's GATConv projects the edge attributes through a linear
    layer and adds them to the attention logits before the softmax.

    Each layer's output dimension is 64 * 4 = 256 (concatenation of 4 heads,
    each producing 64-dimensional output). We chose concatenation over
    averaging because concatenation preserves the distinct information from
    each head, which is important for our multi-head scoring setup downstream.

LayerNorm:
    Applied after each GAT layer to stabilize training. We use LayerNorm
    rather than BatchNorm because graph sizes vary across the batch (157 nodes
    on average but with significant variance), and BatchNorm's statistics would
    be noisy with variable-size inputs.

ELU Activation:
    Exponential Linear Unit, chosen over ReLU because ELU has non-zero
    gradients for negative inputs, which helps with the vanishing gradient
    problem in deep GNNs. GAT's original paper uses ELU.

Residual Connections:
    Each layer adds its output to its input (h = h + h_residual). This is
    critical for 3-layer GNNs because information must propagate at least 3
    hops to go from a stair on floor 1 through the stair-link edge to a
    stair on floor 2. Without residuals, the gradients in the first layer
    would be very small, leading to poor learning.

Dropout (0.2):
    Applied after each GAT layer and within each scoring head. With only 2,000
    training graphs, regularization is important to prevent overfitting. We use
    a moderate rate of 0.2 rather than the common 0.5 because our graphs are
    already relatively small (157 nodes), and excessive dropout would discard
    too much information per forward pass.

Floor-Aware Global Pooling:
    This is a key design decision. We pool each floor's nodes separately
    (using the floor_mask to filter), then concatenate the two floor
    representations. The pooling for each floor is the concatenation of
    global mean pooling and global max pooling, producing a 512-dimensional
    vector per floor.

    WHY FLOOR-AWARE: If we pooled all nodes together (ignoring which floor they
    belong to), the model would lose the ability to compare floor 1's
    representation against floor 2's representation. The coherence task is
    fundamentally a comparison between two floors, so they must be encoded
    separately before comparison.

    WHY MEAN + MAX: Mean pooling captures the "average" representation of a
    floor (good for global properties like overall wall density). Max pooling
    captures the "most extreme" features (good for detecting violations like
    a single very small door or a completely misaligned stair). Concatenating
    both gives the scoring heads access to both perspectives.

    ALTERNATIVE: Use a cross-attention mechanism between the two floor
    representations instead of concatenation. This would let the model
    explicitly compare corresponding elements across floors. We chose
    concatenation for simplicity and because the cross-floor edges in the graph
    already provide inter-floor information flow.

5 Scoring Heads:
    Each head is a 2-layer MLP: Linear(1024, 128) -> ReLU -> Dropout(0.2) ->
    Linear(128, 1) -> Sigmoid. The sigmoid ensures the output is in [0, 1],
    which can be interpreted as a probability or a percentage.

    WHY SEPARATE HEADS: A single overall score would be a black box. By having
    5 separate heads, each scoring a specific coherence dimension, we get:
    (a) Granular diagnosis: "Stair alignment is 0.84 but egress is only 0.33."
    (b) Targeted refinement: The agent fixes the weakest dimension first.
    (c) Explainability: Stakeholders can see exactly which aspect is failing.

    ALTERNATIVE: A single head with a 5-dimensional output vector. This would
    use fewer parameters but couples the dimensions, making it harder to
    interpret and harder to train (a single loss would need to balance all
    five targets).

8.3 PARAMETER COUNT

    Input projection:     32 * 256 = 8,192
    GAT Layer 1:          256 * 64 * 4 (weights) + attention params ~ 67K
    GAT Layer 2:          Same ~ 67K
    GAT Layer 3:          Same ~ 67K
    LayerNorm (3x):       256 * 2 * 3 = 1,536
    Stair head:           1024 * 128 + 128 + 128 * 1 + 1 = 131,201
    Wall head:            Same = 131,201
    Door head:            Same = 131,201
    Egress head:          Same = 131,201
    Overall head:         Same = 131,201
    ------------------------------------------
    TOTAL:                871,685 trainable parameters

This is a deliberately small model. With only 2,000 training graphs, a larger
model would overfit. The 871K parameter count also ensures the model fits
comfortably in <8 GB VRAM even with batch size 16.


================================================================================
9. STAGE 5: TRAINING STRATEGY
================================================================================

9.1 LOSS FUNCTION

We use a multi-task loss that combines all five scoring heads:

    Total Loss = 1.0 * MSE(stair_pred, stair_true)
               + 1.0 * MSE(wall_pred, wall_true)
               + 0.5 * BCE(door_pred, door_true)
               + 0.5 * BCE(egress_pred, egress_true)
               + 2.0 * MSE(overall_pred, overall_true)

WHY MSE FOR SOME AND BCE FOR OTHERS:
- Stair IoU, wall alignment, and overall coherence are continuous values in
  [0, 1]. MSE (Mean Squared Error) is the standard regression loss for
  continuous targets.
- Door compliance and egress compliance are effectively binary (a door either
  meets the 36-pixel threshold or it does not; a floor either has 2 egress
  paths or it does not). BCE (Binary Cross-Entropy) is the standard
  classification loss for binary targets and produces better gradients near
  0 and 1 than MSE.

WHY THESE WEIGHTS:
- Stair and wall alignment (weight 1.0 each): Core structural metrics that
  directly affect building safety.
- Door and egress compliance (weight 0.5 each): Important but binary, so the
  gradients are already strong (BCE produces sharp gradients near decision
  boundaries). Lower weight prevents these binary losses from dominating
  training early on.
- Overall coherence (weight 2.0): The most important output because it is the
  final decision signal. Higher weight anchors the model's overall behavior.

ALTERNATIVE: Learn the loss weights as parameters (uncertainty weighting from
Kendall et al., 2018). This is more principled but adds complexity and can be
unstable with small datasets.

9.2 OPTIMIZER AND SCHEDULER

- Adam optimizer with learning rate 1e-3 and weight decay 1e-4.
  Adam is the standard choice for GNNs because it adapts the learning rate
  per-parameter, which is helpful when different parts of the model (GAT layers
  vs. scoring heads) have different gradient magnitudes.

- ReduceLROnPlateau scheduler: Reduces learning rate by 50% when validation
  loss plateaus for 10 epochs. Minimum learning rate: 1e-6.

- Early stopping with patience 20: If validation loss does not improve for 20
  epochs, training stops and the best model (by validation loss) is restored.

- Gradient clipping at max_norm=1.0: Prevents exploding gradients, which can
  occur in GNNs when message passing amplifies gradient signals through many
  edges.

9.3 DATA SPLIT

    Training:    1,400 graphs (70%)
    Validation:  300 graphs (15%)
    Test:        300 graphs (15%)

The split is shuffled with SEED=42 for reproducibility. We use a simple random
split rather than stratified splitting because the three pair types (positive,
negative, mixed) are already roughly balanced.

9.4 ACTUAL TRAINING RESULTS

    Epoch  1: Train loss = 0.9966, Val loss = 0.9025 (initial random weights)
    Epoch 10: Train loss = 0.3392, Val loss = 0.5389 (rapid initial learning)
    Epoch 20: Train loss = 0.1877, Val loss = 0.1800 (convergence begins)
    Epoch 30: Train loss = 0.1562, Val loss = 0.1563 (near convergence)
    Epoch 40: Train loss = 0.1594, Val loss = 0.1680 (slight overfitting begins)
    Epoch 50: Train loss = 0.1287, Val loss = 0.1706 (LR reduced to 5e-4)
    Epoch 58: EARLY STOP (patience=20 exhausted)
    Best validation loss: 0.1563 (achieved at epoch 30)

    Training time per epoch: ~3.6 seconds on Tesla T4
    Total training time: ~3.5 minutes

KEY OBSERVATIONS:

(a) The model converges quickly (within 20-30 epochs), which is expected
    given the small dataset and synthetic ground truth.

(b) There is mild overfitting after epoch 30 (training loss continues to
    decrease while validation loss plateaus). The early stopping mechanism
    correctly restores the epoch-30 model.

(c) The learning rate was reduced once (epoch ~45, from 1e-3 to 5e-4) but
    this did not significantly improve validation loss, confirming that the
    model had already converged.

(d) The final best validation loss of 0.1563 corresponds to an average per-head
    error of roughly 0.03 (since the total loss is a weighted sum of 5 heads).
    This means each head is off by about 3 percentage points on average, which
    is quite good for a prototype.


================================================================================
9b. STAGE 6b: POST-TRAINING DIAGNOSTIC
================================================================================

After training, we must verify that the GNN has actually learned CONTINUOUS
distance/alignment relationships, not just binary "good vs bad" classification.
This is critical because the Refinement Agent (Section 10) uses the GNN as a
scoring oracle: it makes small geometric changes and checks if the score
improves. If the GNN only outputs binary values (e.g., always 0.9 or 0.1),
the Refiner gets no useful "colder/warmer" gradient signal and just moves
rooms randomly.

9b.1 WHY THIS DIAGNOSTIC EXISTS

The core failure mode identified is: "Stair at (10,10) on F1 + Stair at
(50,50) on F2 = BAD, but the GNN cannot distinguish this from Stair at
(10,10) on F1 + Stair at (15,15) on F2 = SLIGHTLY BAD." If both get the
same low score, the Refiner cannot tell which direction to move the stair.

9b.2 THE THREE SWEEP TESTS

We perform controlled sweep experiments where a single geometric variable
is changed across 20 steps while all other variables are held constant.
For each step, we record the ground-truth metric and the model's predicted
score, then compute statistical correlation.

SWEEP 1 -- STAIR ALIGNMENT:
    Variable: shift_frac from 0.0 (perfect alignment) to 1.0 (100px offset)
    Ground truth: Stair IoU (continuous, 0 to 1)
    Model output: stair_score head prediction
    Expected: As shift increases, GT IoU decreases and predicted score should
    also decrease monotonically.

SWEEP 2 -- WALL ALIGNMENT:
    Variable: remove_frac from 0.0 (all walls intact) to 0.4 (40% removed)
    Ground truth: Wall alignment fraction (continuous, 0 to 1)
    Model output: wall_score head prediction
    Expected: As more walls are removed, GT alignment decreases and predicted
    score should also decrease.

SWEEP 3 -- DOOR COMPLIANCE:
    Variable: shrink_frac from 0.0 (normal size) to 1.0 (shrunk to 30%)
    Ground truth: Door compliance fraction (effectively binary per door, but
    continuous in aggregate)
    Model output: door_score head prediction
    Expected: As doors shrink below the 36-pixel threshold, GT compliance
    drops and predicted score should follow.

9b.3 STATISTICAL METRICS

For each sweep, we compute:

SPEARMAN RANK CORRELATION (rho):
    Measures whether the predicted scores preserve the RANK ORDER of ground
    truth values. Unlike Pearson correlation, Spearman does not assume a
    linear relationship -- it only requires monotonicity. Rho ranges from
    -1 (perfect inverse) to +1 (perfect agreement).

    PASS threshold: rho >= 0.7 for continuous heads (stair, wall)
                    rho >= 0.5 for binary heads (door, egress)

MONOTONICITY FRACTION:
    The fraction of consecutive sweep steps where the predicted score changes
    in the same direction as the ground truth. A monotonicity of 100% means
    the model perfectly tracks every incremental geometric change. Even a
    model with good Spearman correlation may have local non-monotonic regions
    that cause the Refiner to oscillate.

9b.4 SCATTER PLOT ANALYSIS

We also generate prediction-vs-ground-truth scatter plots on the full test
set (300 pairs, all 5 heads). For each head:

R-SQUARED (R2):
    Measures how much of the ground-truth variance is explained by the model's
    predictions. R2 = 1 means perfect prediction, R2 = 0 means the model
    predicts the mean for everything. Negative R2 means worse than the mean.

Ideal scatter plots show points clustered along the diagonal (y=x line).
If points cluster into two horizontal bands (e.g., all predictions near
0.1 or 0.9), it confirms binary collapse.

9b.5 PASS/FAIL INTERPRETATION

The diagnostic prints a per-head PASS/FAIL table:

    If ALL PASS: The GNN has learned continuous distance relationships.
    The Refiner WILL get useful gradient signals.

    If SOME FAIL: The GNN may collapse to binary classification for those
    dimensions. The Refiner will randomly move rooms for failed dimensions.
    Remediation: increase sweep training data ratio, raise loss weights
    for failed heads, or train for more epochs.

9b.6 OUTPUT FILES

    diagnostic_scatter.png  -- 5-panel scatter plot (pred vs GT, all heads)
    diagnostic_sweeps.png   -- 3-panel sweep plot (stair, wall, door)


================================================================================
10. STAGE 6: THE REFINEMENT AGENT
================================================================================

10.1 HOW IT WORKS

The refinement agent is a 3-iteration loop that uses the trained GNN model
as a scoring oracle and applies targeted heuristic fixes based on the weakest
coherence dimension:

    for iteration in 0, 1, 2:
        1. Score the current pair using the GNN model (forward pass)
        2. If overall_score >= 0.8: STOP (the pair is coherent)
        3. Identify which of the 4 sub-heads has the lowest score
        4. Apply the corresponding heuristic fix
        5. Log the action taken and the before/after scores

10.2 THE FOUR HEURISTIC FIXES

FIX: SHIFT STAIR (triggered when stair_score is weakest)
    Compute the centroid offset between the lower stair and upper stair.
    Shift the upper stair by 50% of this offset toward the lower stair's
    position. We use 50% (not 100%) as a conservative step to avoid
    overshooting. Over 3 iterations with 50% steps, the stair converges to
    within 12.5% of perfect alignment (0.5^3 = 0.125 remaining error).

    Example from actual run:
        Iteration 0: Shifted by (-27.9, 27.9)px. Stair IoU: 0.663 -> 0.812
        Iteration 1: Shifted by (-14.0, 14.0)px. Stair IoU: 0.812 -> 0.900
        Iteration 2: Shifted by (-7.0, 7.0)px.   Stair IoU: 0.900 -> 0.948

FIX: SNAP WALLS (triggered when wall_score is weakest)
    For each misaligned upper-floor wall (0.1 < IoU < 0.8 with best lower-floor
    match), shift it to match the lower wall's centroid. Only snap if the
    distance is less than 50 pixels (to avoid snapping walls that genuinely
    belong in different positions).

FIX: RESIZE DOORS (triggered when door_score is weakest)
    Find all doors with opening width below 36 pixels. Scale them up uniformly
    around their centroid to achieve 38 pixels (36 minimum + 2 pixel margin).

FIX: ADD EGRESS DOOR (triggered when egress_score is weakest)
    Find the longest outer-perimeter wall that does not have an adjacent door.
    Insert a new door polygon (40px wide, 10px thick) at the wall's midpoint.
    Add it to the polygon dictionary and the outer perimeter list.

10.3 WHY HEURISTICS AND NOT LEARNED REFINEMENT

The refinement agent uses hand-coded heuristics rather than a learned policy
(e.g., reinforcement learning) for three reasons:

(a) INTERPRETABILITY: Every action has a clear, human-readable explanation.
    "Shifted upper stair by (-27.9, 27.9)px toward lower stair" is immediately
    understandable by an architect. A learned policy's actions would be opaque.

(b) RELIABILITY: Heuristics are deterministic and guaranteed to improve the
    target metric. The stair shift always reduces the centroid offset. The door
    resize always increases the opening width. A learned policy could
    occasionally make things worse.

(c) SIMPLICITY: RL-based refinement would require defining a reward function,
    training a separate policy network, and handling the exploration-exploitation
    trade-off. For a prototype, heuristics are vastly simpler to implement and
    debug.

ALTERNATIVE: Train a Graph Reinforcement Learning (GRL) agent that learns to
modify polygon positions to maximize the GNN's coherence score. This would be
more flexible (it could discover refinement strategies we did not anticipate)
but requires significant additional engineering. This is listed in the
production roadmap (Section 17).

10.4 WHY 3 ITERATIONS

Three iterations provide diminishing returns:
- Iteration 1 fixes the largest violation (often improving the score by 10-20%)
- Iteration 2 fine-tunes (5-10% improvement)
- Iteration 3 polishes (1-5% improvement)

Additional iterations would provide marginal benefit because the 50% step size
means the remaining error is halved each time. After 3 iterations, 87.5% of
the original error has been corrected.


================================================================================
11. STAGE 7: EVALUATION METHODOLOGY
================================================================================

11.1 WHAT WE MEASURE

We evaluate on the BROKEN subset of the test set (pairs with label < 0.9),
which represents naively stacked floors with structural violations. This is
the realistic use case: the system receives a potentially incoherent multi-story
stacking and must assess and fix it.

BASELINE: The ground-truth coherence metrics of the broken test pairs BEFORE
any refinement. This represents what you get by naively stacking two floors
with no intelligence.

GNN AGENT: The ground-truth coherence metrics of the same pairs AFTER running
the 3-iteration refinement loop. This represents the system's ability to
identify and fix violations.

Note: We report ground-truth metrics (analytically computed from the actual
polygon geometry), not model-predicted scores. This prevents the evaluation
from being circular (the model scoring its own output). The ground-truth
metrics are the actual stair IoU, wall alignment fraction, door compliance
fraction, and egress count.

11.2 WHY NOT EVALUATE ON ALL PAIRS

Positive pairs (label > 0.9) are already coherent by construction. Including
them in the evaluation would inflate both baseline and GNN agent scores,
masking the system's ability to fix violations. By evaluating only on broken
pairs, we directly measure the value added by the GNN + refinement pipeline.

The test set contains 300 total pairs, of which 142 are broken (label < 0.9).
This gives us a statistically meaningful sample.


================================================================================
12. COMPLETE RESULTS AND ANALYSIS
================================================================================

12.1 MAIN EVALUATION TABLE

    | Method     | Egress % | Alignment % | Overall % | Fix Rate |
    |------------|----------|-------------|-----------|----------|
    | Baseline   | 33       | 78          | 69        | -        |
    | GNN Agent  | 76       | 78          | 80        | +16%     |

Fix Rate is computed as: (GNN_overall - Baseline_overall) / Baseline_overall
= (80 - 69) / 69 = +16%

12.2 PER-HEAD SCORE BREAKDOWN

    BASELINE (Before Refinement):
    +----------------------+----------+
    | Head                 | Score    |
    +----------------------+----------+
    | Stair Alignment      |   72.9%  |
    | Wall Alignment       |   78.4%  |
    | Door Compliance      |   91.8%  |
    | Egress Compliance    |   33.1%  |
    | Overall Coherence    |   69.1%  |
    +----------------------+----------+

    GNN AGENT (After Refinement):
    +----------------------+----------+
    | Head                 | Score    |
    +----------------------+----------+
    | Stair Alignment      |   84.4%  |
    | Wall Alignment       |   78.4%  |
    | Door Compliance      |   81.7%  |
    | Egress Compliance    |   76.1%  |
    | Overall Coherence    |   80.1%  |
    +----------------------+----------+

12.3 DETAILED ANALYSIS OF EACH DIMENSION

STAIR ALIGNMENT: 72.9% -> 84.4% (+11.5 percentage points)
    The refinement agent successfully improved stair alignment by iteratively
    shifting the upper stair toward the lower stair. The 50% step size
    converges rapidly: a typical example went from IoU 0.663 -> 0.812 -> 0.900
    -> 0.948 across three iterations. The remaining gap to 100% is because
    some pairs have stairs shifted so far (100 pixels) that three 50% steps
    cannot fully close the gap. More iterations or larger step sizes would
    help but were not needed to exceed the 0.8 threshold.

WALL ALIGNMENT: 78.4% -> 78.4% (no change)
    Wall alignment did not improve because in most broken pairs, the wall
    removal (20-40% of walls deleted) creates a fundamental mismatch that
    cannot be fixed by snapping remaining walls. You cannot align a wall that
    does not exist. This is a limitation of the current heuristic: it can
    only snap existing walls, not reconstruct missing ones.

    WHY THE BASELINE IS ALREADY HIGH (78.4%): Even in negative pairs, we only
    remove 20-40% of walls. The remaining 60-80% of walls are still in their
    original positions from the lower floor copy, so they align perfectly. The
    78.4% figure reflects this: roughly 80% of walls are still aligned because
    they were never moved, only some were deleted.

DOOR COMPLIANCE: 91.8% -> 81.7% (-10.1 percentage points)
    Door compliance actually DECREASED after refinement. This is because the
    refinement agent prioritizes the weakest head (usually stair or egress),
    and the door resize heuristic is rarely triggered. Meanwhile, adding
    egress doors (the egress fix) introduces new small doors that may not all
    meet the 36-pixel minimum. The door-creation heuristic creates doors at
    a fixed 40x10 pixel size, which has an opening of 10 pixels (the shorter
    dimension), below the 36-pixel threshold.

    This is a bug in the egress fix heuristic: when we add a new door for
    egress, we should ensure its opening width (not just its total width)
    exceeds 36 pixels. Specifically, the door should be 40x40 (square) or
    the opening dimension should be at least 36 pixels. This is a fixable
    issue and is noted in the production roadmap.

EGRESS COMPLIANCE: 33.1% -> 76.1% (+43.0 percentage points)
    This is the largest improvement and the most impactful. The baseline
    egress compliance is very low (33.1%) because the wall-removal and
    door-shrinkage perturbations in negative pairs often eliminate exterior
    doors. The refinement agent's egress fix adds new doors on the outer
    perimeter, dramatically improving egress compliance.

    The improvement from 33.1% to 76.1% is a 130% relative improvement
    (43.0 / 33.1 = 1.30). This is the single most valuable contribution
    of the refinement agent.

    Why not 100%: Some floor plans have very few outer-perimeter walls
    identified in the SVG data. If the outer_perimeter_ids list is incomplete
    or missing, the egress fix cannot find walls to add doors to. This is a
    dataset annotation limitation.

OVERALL COHERENCE: 69.1% -> 80.1% (+11.0 percentage points)
    The overall score is the average of the four sub-scores:
    Baseline: (72.9 + 78.4 + 91.8 + 33.1) / 4 = 69.1%
    GNN Agent: (84.4 + 78.4 + 81.7 + 76.1) / 4 = 80.1%

    The +16% relative improvement (11 / 69.1) demonstrates that the GNN +
    refinement pipeline provides meaningful value over naive stacking.

12.4 WHAT THE RESULTS MEAN FOR DRAFTED.AI

The prototype demonstrates that:

(a) It is feasible to automatically score multi-story floor plan coherence
    using a GNN trained on synthetic data derived from single-story plans.

(b) A simple iterative refinement agent can fix the most common violations
    (stair misalignment, missing egress) and bring overall coherence above
    the 80% threshold.

(c) The system provides per-dimension explanations that architects and
    engineers can audit and trust.

(d) The entire pipeline runs in under 12 minutes on a single T4 GPU,
    including data generation, training, evaluation, and visualization. In
    production, inference (scoring a single pair + refinement) takes less
    than 1 second.

12.5 TRAINING PERFORMANCE METRICS

    Total training time:           ~3.5 minutes (58 epochs)
    Best validation loss:          0.1563 (epoch 30)
    Final training loss:           0.1287 (epoch 50, before early stop)
    Convergence speed:             90% of learning in first 20 epochs
    Learning rate reductions:      1 (from 1e-3 to 5e-4 at epoch ~45)
    Model checkpoint size:         3.5 MB (best_model.pt)
    Peak GPU memory:               < 2 GB (well under 8 GB constraint)
    Time per epoch:                3.6 seconds

12.6 MODEL INFERENCE PERFORMANCE

    Single pair scoring time:      ~5 ms (forward pass only)
    Refinement loop (3 iters):     ~20 ms (3 forward passes + heuristics)
    Graph construction:            ~50 ms per pair
    Total inference per pair:      ~70 ms


================================================================================
13. VISUALIZATION OUTPUTS
================================================================================

All visualizations are saved to /home/sujals2144/drafted/outputs/.

13.1 TRAINING CURVES (training_curves.png)

A 2x3 subplot grid showing training and validation loss curves for:
- Total loss (upper left)
- Stair alignment loss (upper center)
- Wall alignment loss (upper right)
- Door compliance loss (lower left)
- Egress compliance loss (lower center)
- Overall coherence loss (lower right)

Blue lines = training loss, Red lines = validation loss. All heads show
convergence within 20-30 epochs, with mild overfitting visible after epoch 30.

13.2 PAIR PLOTS (pair_0.png, pair_1.png, pair_2.png)

Each plot shows:
- Top left: Floor 1 (Lower) with all polygons drawn in their type colors
  (walls in light blue, rooms in teal with 30% opacity, doors in orange,
  windows in yellow, stair in purple with hatching)
- Top center: Floor 2 (Upper) with the same color scheme
- Top right: Legend mapping colors to element types
- Bottom left: Radar chart with 5 axes (Stair Align, Wall Align, Door Comply,
  Egress, Overall) showing the coherence scores as a filled polygon
- Bottom right: Score table with numeric values and color-coded PASS/FAIL
  status (green for >0.5, red for <=0.5)

13.3 ATTENTION HEATMAPS (attention_0.png, attention_1.png, attention_2.png)

The primary explainability visualization. Shows:
- Left panel: Floor 1 with graph edges drawn as colored lines
- Right panel: Floor 2 with graph edges drawn as colored lines
- Edge color: Yellow (low attention) to Red (high attention) using the
  YlOrRd colormap
- Colorbar: Maps attention weight values to colors

These plots show WHERE the model is "looking" when scoring coherence. High-
attention edges between structurally important elements (stair-to-stair,
wall-to-wall across floors) indicate the model has learned meaningful structural
reasoning.

13.4 INTERACTIVE SVGS (interactive_0.svg, interactive_1.svg, interactive_2.svg)

SVG files that can be opened in a web browser. Features:
- All floor plan polygons drawn with correct colors and opacity
- CSS hover effects: mousing over any polygon highlights it in red
- Tooltip text showing polygon class and ID appears on hover
- Stair polygons have a pulsing red animation (CSS @keyframes)
- Score dashboard at the bottom with color-coded values

These are useful for stakeholder demos where the audience needs to interactively
explore the floor plans.

13.5 REFINEMENT GIFS (refinement_0.gif, refinement_1.gif, refinement_2.gif)

Animated GIFs showing the refinement process frame by frame:
- Frame 1: Initial (broken) state with current scores
- Frames 2-4: State after each refinement iteration
- Score bar at the bottom shows all 5 scores updating in real time

These animations demonstrate the system's ability to iteratively fix violations,
making the refinement process tangible and understandable.


================================================================================
14. EXPLAINABILITY DEEP DIVE
================================================================================

Explainability is a first-class concern in this project because floor plan
decisions affect building safety and must be auditable. The system provides
explainability at five levels:

LEVEL 1: PER-HEAD SCORE BREAKDOWN
    Instead of a single opaque "coherence score," the system provides 5
    independent scores. An architect can immediately see: "Stair alignment is
    fine (84%) but egress is failing (33%)." This granularity enables targeted
    investigation and fixes.

LEVEL 2: ATTENTION WEIGHT VISUALIZATION
    The GAT model computes attention weights for every edge at every layer.
    We extract the last layer's attention weights, average across the 4 heads,
    and overlay them on the floor plan geometry. High-attention edges reveal
    which structural relationships the model considers most important for its
    scoring decision.

    For example, in a pair with poor stair alignment, we expect to see high
    attention on the stair-link edge (connecting floor 1's stair to floor 2's
    stair) and on edges connecting the stair to surrounding walls. This confirms
    the model is reasoning about stair position, not just memorizing labels.

LEVEL 3: REFINEMENT ACTION LOG
    Every heuristic action is logged as a RefinementAction dataclass with:
    - action_type: What kind of fix ("shift_stair", "realign_wall", etc.)
    - target_id: Which polygon was modified
    - description: Human-readable explanation in natural language
    - delta: Exact numeric change (e.g., {"dx": -27.9, "dy": 27.9})
    - score_before: The relevant sub-score before the fix
    - score_after: The relevant sub-score after the fix

    Example log entry:
    "Shifted upper stair by (-27.9, 27.9)px toward lower stair.
     Stair IoU: 0.663 -> 0.812"

    This level of detail allows an architect to verify that every automated
    fix is reasonable and to override or undo specific actions if needed.

LEVEL 4: RADAR CHART DASHBOARD
    A radar chart with 5 axes provides an at-a-glance visual summary of
    coherence. The filled area represents the "quality envelope" -- a larger
    area means better overall coherence. The shape reveals imbalances: a
    spike toward one axis with a dip toward another immediately shows which
    dimension is strong and which is weak.

LEVEL 5: ANIMATED REFINEMENT PROCESS
    The refinement GIFs show the floor plan geometry changing frame by frame
    as violations are fixed. This makes the abstract concept of "iterative
    refinement" concrete and visual. Stakeholders can literally watch the
    stair slide into alignment or a new egress door appear.


================================================================================
15. EVERY DESIGN DECISION AND ITS ALTERNATIVES
================================================================================

This section catalogs every significant design decision in the project,
explains why we made it, and describes what alternatives exist.

DECISION 1: REGEX-BASED SVG PARSING
    Choice: Parse SVG files using Python regex (re.finditer)
    Why: CVC-FP SVGs contain non-standard XML tags that break standard parsers
    Alternative: Preprocess SVGs to valid XML, then use lxml
    Trade-off: Regex is fragile but works for this fixed dataset

DECISION 2: SYNTHETIC STAIR INJECTION
    Choice: Place synthetic stairs in the largest room
    Why: Zero actual stair polygons exist in the dataset
    Alternative: Place stairs at room intersections (near doors connecting
    multiple rooms) for more architectural realism
    Trade-off: Largest-room placement is simpler and guaranteed to fit

DECISION 3: 2000 SYNTHETIC PAIRS
    Choice: Generate 2000 multi-story pairs from 122 single-floor layouts
    Why: Each layout is reused ~16 times with different random perturbations,
    providing sufficient variety without excessive training time
    Alternative: Generate 10,000+ pairs for better generalization
    Trade-off: More pairs would require longer training and more memory

DECISION 4: 33/34/33 PAIR RATIO
    Choice: Roughly equal positive/negative/mixed pairs
    Why: Prevents the model from learning binary shortcuts
    Alternative: Oversample negatives (since violations are the primary concern)
    Trade-off: Balanced sampling is simpler; oversampling would emphasize
    violation detection at the cost of coherence recognition

DECISION 5: GAT OVER GCNConv
    Choice: Graph Attention Network
    Why: Attention weights provide interpretability; edge_dim support enables
    edge-type-aware attention
    Alternative: GCNConv (simpler, faster), GraphSAGE (better for inductive
    learning), Graph Transformer (more expressive)
    Trade-off: GAT is moderately complex but uniquely suited for our
    interpretability requirements

DECISION 6: 3 GAT LAYERS
    Choice: 3 layers with 4 heads each
    Why: 3 hops is the minimum for information to flow from floor 1's stair
    through the stair-link edge to floor 2's stair
    Alternative: 2 layers (insufficient propagation), 4+ layers (over-smoothing
    risk where all node representations converge to the same vector)
    Trade-off: 3 layers is the sweet spot for our graph diameter

DECISION 7: 64-DIM HIDDEN, 4 HEADS
    Choice: Hidden dimension 64 per head, 4 heads (total 256)
    Why: Keeps the model under 1M parameters to avoid overfitting on 2000 graphs
    Alternative: 128-dim, 8 heads for more capacity
    Trade-off: Larger model would likely overfit; 64/4 is sufficient

DECISION 8: FLOOR-AWARE POOLING
    Choice: Pool each floor separately, then concatenate
    Why: The coherence task is fundamentally a comparison between two floors
    Alternative: Single global pool (loses floor distinction), cross-attention
    between floor representations (more expressive but complex)
    Trade-off: Concatenation is simple and effective

DECISION 9: MEAN + MAX POOLING
    Choice: Concatenate global mean pool and global max pool
    Why: Mean captures global statistics, max captures extremes (violations)
    Alternative: Only mean pool, only max pool, attention-weighted pool
    Trade-off: Concatenation doubles the pooling output dimension but provides
    complementary information

DECISION 10: 5 SEPARATE SCORING HEADS
    Choice: Independent 2-layer MLP for each coherence dimension
    Why: Enables per-dimension diagnosis, targeted refinement, and explainability
    Alternative: Single head with 5-dim output (fewer parameters, coupled dims)
    Trade-off: 5 heads use more parameters (~655K of 871K total) but provide
    essential interpretability

DECISION 11: MSE + BCE MULTI-TASK LOSS
    Choice: MSE for continuous scores, BCE for binary compliance
    Why: Standard loss functions matched to target type
    Alternative: All MSE (simpler), Focal Loss (better for imbalanced binary
    targets), learned uncertainty weighting
    Trade-off: Our choice is standard and well-understood

DECISION 12: ADAM OPTIMIZER
    Choice: Adam with lr=1e-3, weight_decay=1e-4
    Why: Standard choice for GNNs, adapts per-parameter learning rates
    Alternative: SGD with momentum (may generalize better but requires careful
    LR scheduling), AdamW (decouples weight decay from gradient update)
    Trade-off: Adam converges faster, which matters for rapid prototyping

DECISION 13: HEURISTIC REFINEMENT OVER LEARNED POLICY
    Choice: Hand-coded fix heuristics (stair shift, wall snap, door resize,
    egress addition)
    Why: Deterministic, interpretable, guaranteed to improve target metric
    Alternative: RL-based refinement agent, differentiable optimization
    Trade-off: Heuristics cannot discover novel fix strategies; RL could but
    requires significant additional engineering

DECISION 14: 50% STEP SIZE FOR STAIR REFINEMENT
    Choice: Shift stair by 50% of remaining offset each iteration
    Why: Conservative step prevents overshooting; converges to 87.5% correction
    in 3 iterations
    Alternative: 100% step (immediate correction but may overshoot if the model
    scores change nonlinearly), learned step size
    Trade-off: 50% is safe; 100% would be faster but riskier

DECISION 15: 10% DIAGONAL THRESHOLD FOR CROSS-FLOOR EDGES
    Choice: Connect nodes across floors if distance < 10% of canvas diagonal
    Why: Approximately matches the spatial range within which same-type elements
    should correspond across floors
    Alternative: 5% (fewer edges, may miss valid correspondences), 20% (more
    edges, introduces noise from non-corresponding elements)
    Trade-off: 10% is a reasonable middle ground for our canvas sizes

DECISION 16: 32-DIM NODE FEATURES
    Choice: 32 dimensions encoding type, geometry, position, and class-specific
    features
    Why: Compact enough to avoid overfitting, expressive enough to encode all
    relevant properties
    Alternative: 64 or 128 dimensions with additional features (perimeter,
    convexity, orientation angle)
    Trade-off: Diminishing returns beyond 32 dims for our dataset size

DECISION 17: ONE-HOT EDGE TYPES (7-DIM)
    Choice: 7-dimensional one-hot vector for edge attributes
    Why: Treats each edge type as an independent category, no ordinal bias
    Alternative: Scalar encoding (0-6), learned embedding per type
    Trade-off: One-hot is the simplest correct encoding; learned embeddings
    would add parameters and complexity

DECISION 18: SEED = 42
    Choice: All random operations use seed 42
    Why: Full reproducibility across runs
    Alternative: Any other integer; 42 is conventional (The Hitchhiker's Guide)


================================================================================
16. LIMITATIONS OF THE CURRENT PROTOTYPE
================================================================================

LIMITATION 1: SYNTHETIC DATA ONLY
    The model is trained entirely on synthetic multi-story pairs derived from
    single-floor CVC-FP plans. It has never seen real multi-story floor plans.
    This means it may not generalize to real-world multi-story buildings that
    have floor-specific features (e.g., a ground floor with a garage and an
    upper floor with bedrooms).

LIMITATION 2: NO ACTUAL STAIRS IN TRAINING DATA
    All stairs are synthetically injected. Real stairs have specific structural
    requirements (headroom, riser height, tread depth) that our simple
    rectangular stair polygons do not capture.

LIMITATION 3: WALL REFINEMENT CANNOT RECONSTRUCT DELETED WALLS
    The wall-snapping heuristic can only realign existing walls. If walls are
    missing from the upper floor (as in our negative pair generation), the
    heuristic cannot reconstruct them. This explains why wall alignment shows
    no improvement (78.4% -> 78.4%).

LIMITATION 4: EGRESS DOOR CREATION BUG
    The egress fix creates doors with a 40x10 pixel size. The opening width
    is the shorter dimension (10 pixels), which is below the 36-pixel minimum.
    This causes door compliance to decrease when egress doors are added. The
    fix is simple: ensure the shorter dimension of new doors is at least 36
    pixels.

LIMITATION 5: SMALL DATASET
    122 floor plans is a small dataset. While we generate 2000 synthetic pairs,
    the underlying floor plan diversity is limited. The model may overfit to
    the specific room layouts and wall patterns in CVC-FP.

LIMITATION 6: NO LOAD-BEARING ANALYSIS
    The system does not distinguish between load-bearing and non-load-bearing
    walls. In real buildings, only load-bearing walls need to align across
    floors. Treating all walls equally inflates the wall alignment metric.

LIMITATION 7: 2D ONLY
    The system works in 2D (polygon bounding boxes). It does not model 3D
    structural properties like floor-to-ceiling height, beam locations, or
    foundation loads.

LIMITATION 8: SINGLE BUILDING TYPE
    CVC-FP contains mostly residential floor plans. The model may not
    generalize to commercial, industrial, or mixed-use buildings.

LIMITATION 9: NO PLUMBING OR ELECTRICAL ALIGNMENT
    Multi-story buildings require vertical alignment of plumbing stacks and
    electrical runs, which we do not model.

LIMITATION 10: REFINEMENT IS HEURISTIC, NOT OPTIMAL
    The greedy approach (fix the weakest dimension first) may not find the
    globally optimal refinement sequence. For example, fixing stair alignment
    first might make wall alignment worse if the stair shift moves the stair
    onto a wall.


================================================================================
17. PRODUCTION READINESS ROADMAP
================================================================================

To take this prototype to production, the following improvements are needed,
organized by priority:

PRIORITY 1: CRITICAL FIXES (1-2 weeks)

(a) Fix the egress door creation bug: Ensure new egress doors have an opening
    width of at least 36 pixels. Change the door creation dimensions from 40x10
    to 40x40 or ensure the shorter dimension is the wall thickness (10px) and
    the longer dimension (40px) is the opening width.

(b) Add wall reconstruction to the refinement agent: When walls are missing
    on the upper floor, copy the corresponding wall from the lower floor
    instead of just snapping existing walls.

(c) Integrate the ROBIN/ROBIN++ dataset for additional training data. These 510
    floor plans can be vectorized using a segmentation model (like the Dodge et
    al. FCN approach) to produce polygon annotations, increasing the dataset to
    632 plans and approximately 10,000+ synthetic pairs.

PRIORITY 2: MODEL IMPROVEMENTS (2-4 weeks)

(d) Replace synthetic stair injection with a learned stair placement model.
    Train a small network to predict optimal stair locations based on room
    connectivity, hallway positions, and building footprint.

(e) Add load-bearing wall classification. Train a binary classifier to
    distinguish load-bearing from non-load-bearing walls based on wall
    thickness, position (exterior vs. interior), and length. Only require
    alignment for load-bearing walls.

(f) Implement learned refinement via Graph Reinforcement Learning. Define a
    Markov Decision Process where states are floor plan geometries, actions are
    polygon modifications (shift, scale, insert, delete), and rewards are the
    GNN coherence scores. Train a policy network (e.g., PPO or SAC) to learn
    optimal refinement sequences.

(g) Add a 3D awareness module. Extend node features to include floor index (0
    for ground, 1 for first floor, etc.), ceiling height, and vertical
    structural load estimates. This enables the model to reason about 3D
    structural integrity.

(h) Use a Heterogeneous GNN (HeteroConv) instead of encoding node/edge types
    as features. This is more principled for graphs with multiple node and edge
    types and typically achieves better performance.

PRIORITY 3: DATA AND EVALUATION (4-8 weeks)

(i) Collect real multi-story floor plan data. Partner with architectural firms
    or use public building permit databases to obtain paired floor plans from
    actual multi-story buildings. Even 50-100 real multi-story plans would
    significantly improve evaluation credibility.

(j) Implement IBC/IRC code compliance checking. Replace the simple threshold-
    based checks (36-pixel doors, 2 egress paths) with actual International
    Building Code and International Residential Code rules. This includes
    stair riser/tread dimensions, minimum corridor widths, fire separation
    requirements, and accessible route requirements.

(k) Add plumbing stack alignment scoring. Bathrooms and kitchens on upper floors
    should align with bathrooms and kitchens on lower floors for efficient
    plumbing routing. Train a head to score this alignment.

(l) Implement A/B testing framework. Compare the GNN-refined floor plans against
    human architect assessments to quantify real-world improvement.

PRIORITY 4: DEPLOYMENT AND SCALING (8-12 weeks)

(m) Build an inference API (FastAPI or similar) that accepts a pair of floor plan
    SVGs and returns coherence scores + refinement suggestions in JSON format.
    Target latency: <100ms per pair.

(n) Implement model versioning and automated retraining pipeline. As new training
    data arrives, automatically retrain the model and compare against the
    previous version.

(o) Add support for 3+ story buildings. The current system handles 2-floor
    pairs. For N-story buildings, score all consecutive floor pairs and require
    coherence across every pair.

(p) Integrate with the existing Drafted.ai floor plan generation model. Instead
    of scoring coherence after generation, embed the coherence constraints as a
    loss term during generation, ensuring the model produces coherent multi-story
    plans from the start.

(q) Optimize for A100 GPUs and multi-GPU training using DistributedDataParallel
    (not DataParallel). DDP scales linearly with GPU count and avoids the GIL
    bottleneck of DataParallel.

(r) Implement model quantization (INT8) and TorchScript compilation for
    production inference. This can reduce inference latency by 2-4x and model
    size by 4x.

PRIORITY 5: ADVANCED FEATURES (12+ weeks)

(s) Generative multi-story floor plan design. Instead of just scoring and
    refining, train a conditional generative model (e.g., GraphVAE or
    Diffusion-based) that generates a coherent upper floor conditioned on the
    lower floor layout. The GNN coherence scorer serves as a discriminator or
    reward model.

(t) Interactive refinement UI. Build a web-based interface where architects can
    see the floor plans, scores, and attention heatmaps in real time, manually
    adjust elements, and see scores update instantly.

(u) Multi-objective optimization. Instead of a greedy refinement loop, use
    Pareto optimization to find floor plan configurations that simultaneously
    maximize all 5 coherence dimensions while respecting constraints.


================================================================================
18. APPENDIX A: EXACT RUNTIME NUMBERS
================================================================================

All numbers from a single run on the described hardware (Tesla T4, 15.6 GB,
Python 3.10, PyTorch 2.9.1+cu128, CUDA 12.8).

    STAGE                              TIME
    ----------------------------------------
    SVG Parsing (122 files)            < 1s
    Synthetic Pair Generation (2000)   26.8s
    Graph Construction (2000)          108.0s
    Training (58 epochs)               ~210s (~3.5 min)
    Refinement (142 broken pairs)      ~180s (~3 min)
    Visualization (all outputs)        ~30s
    ----------------------------------------
    TOTAL                              ~555s (~9.3 min)

    MODEL STATISTICS
    ----------------------------------------
    Trainable parameters:              871,685
    Checkpoint size:                   3.5 MB
    Peak GPU memory:                   < 2 GB
    Average nodes per graph:           157
    Average edges per graph:           3,978

    TRAINING PROGRESSION
    ----------------------------------------
    Epoch  1: Train=0.9966, Val=0.9025, LR=1.0e-3
    Epoch 10: Train=0.3392, Val=0.5389, LR=1.0e-3
    Epoch 20: Train=0.1877, Val=0.1800, LR=1.0e-3
    Epoch 30: Train=0.1562, Val=0.1563, LR=1.0e-3  [BEST]
    Epoch 40: Train=0.1594, Val=0.1680, LR=1.0e-3
    Epoch 50: Train=0.1287, Val=0.1706, LR=5.0e-4
    Epoch 58: EARLY STOP (patience=20)


================================================================================
19. APPENDIX B: FILE MANIFEST
================================================================================

    INPUT FILES
    ----------------------------------------
    /home/sujals2144/drafted/data/cvc_fp/raw_svgs/*.svg  (122 files)

    SCRIPT
    ----------------------------------------
    /home/sujals2144/drafted/floorplan_gnn.py            (~2150 lines)

    OUTPUT FILES
    ----------------------------------------
    /home/sujals2144/drafted/outputs/best_model.pt       (3.5 MB model checkpoint)
    /home/sujals2144/drafted/outputs/training_curves.png  (training loss plots)
    /home/sujals2144/drafted/outputs/pair_0.png           (example 1 floor plans + scores)
    /home/sujals2144/drafted/outputs/pair_1.png           (example 2 floor plans + scores)
    /home/sujals2144/drafted/outputs/pair_2.png           (example 3 floor plans + scores)
    /home/sujals2144/drafted/outputs/attention_0.png      (attention heatmap example 1)
    /home/sujals2144/drafted/outputs/attention_1.png      (attention heatmap example 2)
    /home/sujals2144/drafted/outputs/attention_2.png      (attention heatmap example 3)
    /home/sujals2144/drafted/outputs/interactive_0.svg    (interactive SVG example 1)
    /home/sujals2144/drafted/outputs/interactive_1.svg    (interactive SVG example 2)
    /home/sujals2144/drafted/outputs/interactive_2.svg    (interactive SVG example 3)
    /home/sujals2144/drafted/outputs/refinement_0.gif     (refinement animation example 1)
    /home/sujals2144/drafted/outputs/refinement_1.gif     (refinement animation example 2)
    /home/sujals2144/drafted/outputs/refinement_2.gif     (refinement animation example 3)
    /home/sujals2144/drafted/outputs/diagnostic_scatter.png (pred vs GT scatter, all 5 heads)
    /home/sujals2144/drafted/outputs/diagnostic_sweeps.png  (stair/wall/door sweep correlations)

    DOCUMENTATION
    ----------------------------------------
    /home/sujals2144/drafted/COMPLETE_PROJECT_DOCUMENTATION.txt  (this file)


================================================================================
20. APPENDIX C: HYPERPARAMETER REFERENCE
================================================================================

    HYPERPARAMETER                  VALUE       NOTES
    ---------------------------------------------------------------------------
    SEED                            42          Full reproducibility
    NODE_FEAT_DIM                   32          Node feature vector size
    HIDDEN_DIM                      64          GAT hidden dimension per head
    NUM_GAT_HEADS                   4           Attention heads per layer
    NUM_GAT_LAYERS                  3           Number of GAT layers
    NUM_EDGE_TYPES                  7           One-hot edge attribute dimension
    LEARNING_RATE                   1e-3        Initial Adam learning rate
    WEIGHT_DECAY                    1e-4        L2 regularization
    EPOCHS                          80          Maximum training epochs
    BATCH_SIZE                      16          Graphs per training batch
    REFINEMENT_LOOPS                3           Max refinement iterations
    COHERENCE_THRESHOLD             0.8         Target for convergence
    STAIR_MIN_AREA_RATIO            0.02        Min stair area / floor area
    STAIR_MAX_AREA_RATIO            0.08        Max stair area / floor area
    WALL_ALIGN_THRESHOLD            0.80        Min IoU for "aligned" walls
    DOOR_MIN_WIDTH_PX               36          Minimum door opening (pixels)
    PERTURBATION_STAIR_SHIFT        (50, 100)   Negative pair stair shift range
    DROPOUT                         0.2         Dropout rate (GAT + heads)
    LR_SCHEDULER_PATIENCE           10          Epochs before LR reduction
    LR_SCHEDULER_FACTOR             0.5         LR reduction multiplier
    LR_MIN                          1e-6        Minimum learning rate
    EARLY_STOP_PATIENCE             20          Epochs before early stopping
    GRADIENT_CLIP_MAX_NORM          1.0         Max gradient norm
    SPATIAL_PROXIMITY_THRESHOLD     10%         Of canvas diagonal, for edges
    CROSS_FLOOR_THRESHOLD           10%         Of canvas diagonal, for matching
    TRAIN_SPLIT                     70%         Training data fraction
    VAL_SPLIT                       15%         Validation data fraction
    TEST_SPLIT                      15%         Test data fraction
    LOSS_WEIGHT_STAIR               1.0         Stair MSE loss weight
    LOSS_WEIGHT_WALL                1.0         Wall MSE loss weight
    LOSS_WEIGHT_DOOR                0.5         Door BCE loss weight
    LOSS_WEIGHT_EGRESS              0.5         Egress BCE loss weight
    LOSS_WEIGHT_OVERALL             2.0         Overall MSE loss weight
    N_SYNTHETIC_PAIRS               2000        Total training pairs
    POSITIVE_PAIR_RATIO             0.33        Coherent pair fraction
    NEGATIVE_PAIR_RATIO             0.34        Incoherent pair fraction
    MIXED_PAIR_RATIO                0.33        Partially coherent fraction
    STAIR_REFINEMENT_STEP           0.50        Fraction of offset corrected
    WALL_SNAP_MAX_DISTANCE          50          Max pixels for wall snapping
    DOOR_RESIZE_MARGIN              2           Pixels above minimum for resize


================================================================================
END OF DOCUMENT
================================================================================
